{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Lab 4 (Solutions)\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, October 5th, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW4.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Last, First]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS 109a or STATS 121a or AC 209a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[First and Land names of students with whom you have collaborated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Variable selection and regularization\n",
    "\n",
    "The data set for this problem is provided in ``dataset_1.txt`` and contains 10 predictors and a response variable.\n",
    "\n",
    "### Part (a): Analyze correlation among predictors\n",
    "- By visually inspecting the data set, do find that some of the predictors are correlated amongst themselves?\n",
    "\n",
    "\n",
    "- Compute the cofficient of correlation between each pair of predictors, and visualize the matrix of correlation coefficients using a heat map. Do the predictors fall naturally into groups based on the correlation values?\n",
    "\n",
    "\n",
    "- If you were asked to select a minimal subset of predictors to build a good regression model, how many predictors will you pick, and which ones will you choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "On visual inspection of the data, it is seen that the predictors 0, 1 and 2 are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAF6CAYAAAAJaaMjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104dea350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.loadtxt('datasets/dataset_1.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "# Split predictors and response\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Compute matrix of correlation coefficients\n",
    "corr_matrix = np.corrcoef(x.T)\n",
    "\n",
    "# Display heat map \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.pcolor(corr_matrix)\n",
    "\n",
    "ax.set_title('Heatmap of correlation matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the predictors fall into **5 natural groups**. To build a regression model, one might pick a total of **5 predictors**, one from each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Selecting minimal subset of predictors\n",
    "\n",
    "- Apply the variable selection methods discussed in class to choose a minimal subset of predictors that yield high prediction accuracy:\n",
    "    \n",
    "    - Exhaustive search\n",
    "    \n",
    "    - Step-wise forward selection\n",
    "    \n",
    "    - Step-wise backward selection  \n",
    "\n",
    "&emsp;&nbsp;&nbsp; In each method, use the Bayesian Information Criterion (BIC) to choose the subset size.\n",
    "\n",
    "- Do the chosen subsets match the ones you picked using the correlation matrix you had visualized in Part (a)?\n",
    "\n",
    "**Note**: You may use the `statsmodels`'s `OLS` module to fit a linear regression model and evaluate BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset by exhaustive search:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "min_bic = 1e10 # set some initial large value for min BIC score\n",
    "best_subset = [] # best subset of predictors\n",
    "\n",
    "# Create all possible subsets of the set of 10 predictors\n",
    "predictor_set = set(range(10)) # predictor set = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "\n",
    "# Repeat for every possible size of subset\n",
    "for size_k in range(10): \n",
    "    # Create all possible subsets of size 'size', \n",
    "    # using the 'combination' function from the 'itertools' library\n",
    "    subsets_of_size_k = it.combinations(predictor_set, size_k + 1) \n",
    "    \n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2 score\n",
    "    best_k_subset = [] # best subset of predictors of size k\n",
    "    \n",
    "    # Iterate over all subsets of our predictor set\n",
    "    for predictor_subset in subsets_of_size_k:    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, predictor_subset]\n",
    "\n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Update max R^2 and best predictor subset of size k\n",
    "        # If current predictor subset has a higher R^2 score than that of the best subset \n",
    "        # we've found so far, remember the current predictor subset as the best!\n",
    "        if(r_squared > max_r_squared): \n",
    "            max_r_squared = r_squared\n",
    "            best_k_subset = predictor_subset[:]\n",
    "                \n",
    "\n",
    "    # Use only the best subset of size k for the predictors\n",
    "    x_subset = x[:, best_k_subset]\n",
    "        \n",
    "    # Fit and evaluate BIC of the best subset of size k\n",
    "    model = OLS(y, x_subset)\n",
    "    results = model.fit()\n",
    "    bic = results.bic\n",
    "    \n",
    "    # Update minimum BIC and best predictor subset\n",
    "    # If current predictor has a lower BIC score than that of the best subset \n",
    "    # we've found so far, remember the current predictor as the best!\n",
    "    if(bic < min_bic): \n",
    "        min_bic = bic\n",
    "        best_subset = best_k_subset[:]\n",
    "    \n",
    "print('Best subset by exhaustive search:')\n",
    "print sorted(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise forward subset selection:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "### Step-wise Forward Selection\n",
    "d = x.shape[1] # total no. of predictors\n",
    "\n",
    "# Keep track of current set of chosen predictors, and the remaining set of predictors\n",
    "current_predictors = [] \n",
    "remaining_predictors = range(d)\n",
    "\n",
    "# Set some initial large value for min BIC score for all possible subsets\n",
    "global_min_bic = 1e10 \n",
    "\n",
    "# Keep track of the best subset of predictors\n",
    "best_subset = [] \n",
    "\n",
    "# Iterate over all possible subset sizes, 0 predictors to d predictors\n",
    "for size in range(d):    \n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "    best_predictor = -1 # set some throwaway initial number for the best predictor to add\n",
    "    bic_with_best_predictor = 1e10 # set some initial large value for BIC score   \n",
    "        \n",
    "    # Iterate over all remaining predictors to find best predictor to add\n",
    "    for i in remaining_predictors:\n",
    "        # Make copy of current set of predictors\n",
    "        temp = current_predictors[:]\n",
    "        # Add predictor 'i'\n",
    "        temp.append(i)\n",
    "                                    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, temp]\n",
    "        \n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "        if(r_squared > max_r_squared):\n",
    "            max_r_squared = r_squared\n",
    "            best_predictor = i\n",
    "            bic_with_best_predictor = results.bic\n",
    "    \n",
    "    # Remove best predictor from remaining list, and add best predictor to current list\n",
    "    remaining_predictors.remove(best_predictor)\n",
    "    current_predictors.append(best_predictor)\n",
    "    \n",
    "    # Check if BIC for with the predictor we just added is lower than \n",
    "    # the global minimum across all subset of predictors\n",
    "    if(bic_with_best_predictor < global_min_bic):\n",
    "        best_subset = current_predictors[:]\n",
    "        global_min_bic = bic_with_best_predictor\n",
    "    \n",
    "print 'Step-wise forward subset selection:'\n",
    "print sorted(best_subset) # add 1 as indices start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise backward subset selection:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "###  Step-wise Backward Selection\n",
    "d = x.shape[1] # total no. of predictors\n",
    "\n",
    "# Keep track of current set of chosen predictors\n",
    "current_predictors = range(d)\n",
    "\n",
    "# First, fit and evaluate BIC using all 'd' number of predictors\n",
    "model = OLS(y, x)\n",
    "results = model.fit()\n",
    "bic_all = results.bic\n",
    "\n",
    "# Set the minimum BIC score, initially, to the BIC score using all 'd' predictors\n",
    "global_min_bic = bic_all\n",
    "# Keep track of the best subset of predictors\n",
    "best_subset = [] \n",
    "\n",
    "# Iterate over all possible subset sizes, d predictors to 1 predictor\n",
    "for size in range(d - 1, 1, -1): # stop before 0 to avoid choosing an empty set of predictors\n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "    worst_predictor = -1 # set some throwaway initial number for the worst predictor to remove\n",
    "    bic_without_worst_predictor = 1e10 # set some initial large value for min BIC score  \n",
    "        \n",
    "    # Iterate over current set of predictors (for potential elimination)\n",
    "    for i in current_predictors:\n",
    "        # Create copy of current predictors, and remove predictor 'i'\n",
    "        temp = current_predictors[:]\n",
    "        temp.remove(i)\n",
    "                                    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, temp]\n",
    "        \n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "        if(r_squared > max_r_squared):\n",
    "            max_r_squared = r_squared\n",
    "            worst_predictor = i\n",
    "            bic_without_worst_predictor = results.bic\n",
    "          \n",
    "    # Remove worst predictor from current set of predictors\n",
    "    current_predictors.remove(worst_predictor)\n",
    "    \n",
    "    # Check if BIC for the predictor we just removed is lower than \n",
    "    # the global minimum across all subset of predictors\n",
    "    if(bic_without_worst_predictor < global_min_bic):\n",
    "        best_subset = current_predictors[:]\n",
    "        global_min_bic = bic_without_worst_predictor\n",
    "    \n",
    "print 'Step-wise backward subset selection:'\n",
    "print sorted(best_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subset of predictors chosen by exhaustive search, backward selection and forward selection agrees with the correlation matrix we visualize in Part (a). That is, the best predictor subset contains five predictors, with **one predictor from each group of correlated predictors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Lasso and Ridge regression\n",
    "\n",
    "- Apply Lasso regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Apply Ridge regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Is there a difference between the model parameters you obtain different and those obtained from Lasso regression? If so, explain why.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Is there anything peculiar that you observe about the coefficients Ridge regression assigns to the first three predictors? Do you observe the same with Lasso regression? Give an explanation for your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:\n",
      "Coefficients: [ 0.          0.          0.04466466 -0.         -0.01760304 -0.         -0.\n",
      "  0.04519336 -0.40460781 -0.22364868]\n",
      "Predictors with non-zero coefficients: [2, 4, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression\n",
    "reg = Lasso_Reg(alpha = 0.01)\n",
    "reg.fit(x, y)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "print 'Lasso:'\n",
    "print 'Coefficients:', coefficients\n",
    "print  'Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subset of predictors chosen by Lasso agrees with the correlation matrix we visualize in Part (a). That is, the best predictor subset contains five predictors, with **one predictor from each group of correlated predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge:\n",
      "Coefficients: [ 0.04353543  0.04353543  0.04353543  0.55217415 -0.19706852 -0.61421737\n",
      "  0.30484213  0.18742866 -0.50083242 -0.35908145]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression: Fit and evaluate \n",
    "reg = Ridge_Reg(alpha = 0.01)\n",
    "reg.fit(x, y)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "print 'Ridge:'\n",
    "print 'Coefficients:', coefficients\n",
    "print 'Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
