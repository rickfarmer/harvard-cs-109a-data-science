{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Homework 7\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, November 9th, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW7.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import KFold\n",
    "import StringIO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Last, First]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS 109a or STATS 121a or AC 209a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[First and Land names of students with whom you have collaborated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Monitoring Land Cover Changes Using Satellite Images\n",
    "In the face of rapid urban development and climate change, it is now more urgent than ever for governments (and other organizations) to have a detailed, accurate and up-to-date picture of land use and land cover, as well as how the land use/cover is changing over time, in order to make effective policy decision to manage and protect natural resources. Building such a comprehensive picture of land use/cover for a large region is extremely difficult. \n",
    "\n",
    "Recent improvements in satellite imagery and image process have allowed for new tools in land use/cover analysis. The following is an image of the change in vegetation cover around Belize from 1975 to 2007:\n",
    "\n",
    "<img src=\"sat.jpg\">\n",
    "\n",
    "In this problem, we will explore how to use classifiers to detect the presence and location of vegetation in satellite images.\n",
    "\n",
    "\n",
    "### Part 1(a): Detecting vegetation in satellite images\n",
    "\n",
    "The following files contain sampled locations from satelite aeriel images: `dataset_1.txt`, ... `dataset_4.txt`. The first two columns contain the normalized latitude and longitude values. The last column indicates whether or not the location contains vegetation, with 1 indicating the presence of vegetaion and 0 indicating otherwise. \n",
    "\n",
    "These small sets of labels are typically generated by hand (that is, locations might be classified based on field studies or by cross-referencing with government databases). Your task is to use the labeled locations to train a model that will predict whether a new location is vegetation or non-vegetation.\n",
    "\n",
    "- Suppose we were asked to write a computer program to automatically identify the vegetation regions on the landscape. How can we use the model fitting algorithms you have studied so far to identify the boundaries of the vegetation regions? In particular, discuss the suitability of the following algorithms for each of the four data sets (**you do not need to evaluate your classifier, build your argument using data and decision boundary visualizations**): \n",
    "    - linear or polynomial linear regression\n",
    "    - linear or polynomial logistic regression\n",
    "    - linear or quadratic discriminant analysis\n",
    "    - decision trees\n",
    "\n",
    "- By a quick visual inspection of each data set, what do you think is the smallest depth decision tree that would provide a good fit of the vegetation boundaries in each case? Does `sklearn`'s decision tree fitting algorithm always provide a good fit for the proposed depth? If not, explain why. **Support your answer with suitable visualization**.\n",
    "\n",
    "We provide you with a function `plot_tree_boundary` to visualize a decision tree model on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  plot_tree_boundary\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      x (array of predictors)\n",
    "#      y (array of labels)\n",
    "#      model (the decision tree you want to visualize, already fitted)\n",
    "#      title (title for plot)\n",
    "#      ax (a set of axes to plot on)\n",
    "# Returns: \n",
    "#      ax (axes with data and decision boundaries)\n",
    "\n",
    "def plot_tree_boundary(x, y, model, title, ax):\n",
    "    # PLOT DATA\n",
    "    ax.scatter(x[y==1,0], x[y==1,1], c='green')\n",
    "    ax.scatter(x[y==0,0], x[y==0,1], c='white')\n",
    "    \n",
    "    # CREATE MESH\n",
    "    interval = np.arange(0,1,0.01)\n",
    "    n = np.size(interval)\n",
    "    x1, x2 = np.meshgrid(interval, interval)\n",
    "    x1 = x1.reshape(-1,1)\n",
    "    x2 = x2.reshape(-1,1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # PREDICT ON MESH POINTS\n",
    "    yy = model.predict(xx)    \n",
    "    yy = yy.reshape((n, n))\n",
    "\n",
    "    # PLOT DECISION SURFACE\n",
    "    x1 = x1.reshape(n, n)\n",
    "    x2 = x2.reshape(n, n)\n",
    "    ax.contourf(x1, x2, yy, alpha=0.1, cmap='Greens')\n",
    "    \n",
    "    # LABEL AXIS, TITLE\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1(b). What is the best splitting criterion for decision trees?\n",
    "Suppose you are given a data set with 100 points in a satellite image, of which 51 are class 1 and 49 are class 0. Consider following two candidate splits for constructing a decision tree: \n",
    "1. [Part 1 = (Class 1: 11, Class 0: 37), Part 2 = (Class 1: 40, Class 0: 12)]\n",
    "2. [Part 1 = (Class 1: 25, Class 0: 48), Part 2 (Class 1: 26, Class 0: 1)]\n",
    "\n",
    "Which of these is a better split according classification error, Gini coefficient, and Entropy criteria? Do the three criteria agree on the best split, or is one better than the other? Support your answer with a concrete explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Loan Risk Assessment\n",
    "In this problem, you are asked by an Unamed National Bank to build a risk assessment model that predicts whether or not it is risky to give a loan to an applicant based on the information provided in their application. Traditionally, loan applications are processed and assessed by hand, but now the bank wants to move to an automated loan processing system. That is, the bank will provide you with loan applications that it has processed in the past for you to build a classifier for risk assessment, going forward, the bank will reject the loan applications from applicants labeled risky and approve the applications that are labeled safe by your model.\n",
    "\n",
    "The relevant training and test sets are provided in the files: `dataset_5_train.txt` and `dataset_5.test.txt`. The training and testing sets are created from both approved and rejected loan applications that the bank has processed by hand in the past. The first 24 columns contain attributes for each applicant gathered from their application, and the last column contains the credit risk assessment with 1 indicating that the customer is a loan risk, and 0 indicating that the customer is not a loan risk. The names of the attributes are provided in the file `dataset_5_description.txt`.\n",
    "\n",
    "\n",
    "### Part 2(a): A simple decision tree model\n",
    "- Fit a simple decision tree of depth 2 to the training set and report its accuracy on the test set. \n",
    "\n",
    "- Interpret the way your model performs risk classifcation. Would you recommend this classifier to Unamed National Bank for making decisions on the loan applications of **real people**? If yes, make an argument for the merrits of this classifer. If no, then make necessary changes to the data set and fit a new classifier that you believe is fair to use in practice, then compare the two classifiers.\n",
    "\n",
    "\n",
    "We have provided you with a function `display_dt` to display the structure of the decision tree in DOT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print decision tree model 'model', already fitted\n",
    "def display_dt(model):\n",
    "    dummy_io = StringIO.StringIO() \n",
    "    tree.export_graphviz(dt, out_file = dummy_io) \n",
    "    print dummy_io.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2(b): An ensemble of decision trees\n",
    "\n",
    "- One way to improve the prediciton accuracy for this task is to use an ensemble of decision trees fitted on random samples, as follows: given a training set of size $n$, sample new training sets uniformly with replacement, and fit a decision tree model on each random sample.\n",
    "\n",
    "  Now, how would you combine the ensemble into a single classifier? There are at lease two ways:\n",
    "\n",
    "   - *Random classifier*: predict using a randomly chosen decision tree from the ensemble\n",
    "   - *Majority classifier*: predict using the majority vote from decision trees in the ensemble\n",
    "   \n",
    "   \n",
    "\n",
    "- We can also fit a *Random Forest* model for our data (`sklearn.ensemble.RandomForestClassifier`).\n",
    "\n",
    "Is there a significant difference in the prediction accuracies of the above three approaches on the loan data set? If so, explain why.\n",
    "\n",
    "\n",
    "**Note:**  The Random Forest approach can easily overfit the training set. What are the important parameters in `sklearn`'s Random Forest fitting function that influence the model fit? For the risk assessment task, you **need** to fit your random forest model by using a suitable model selection procedure to tune these parameters.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Boosting for Classification\n",
    "\n",
    "We've seen in class that boosting is a useful ensemble method to combine a collection of simple regression trees into a powerful regression model. Chapter 10.1 of the text book ([*J.H. Friedman, R. Tibshirani, and T. Hastie, \"The Elements of Statistical Learning\"*](http://statweb.stanford.edu/~tibs/ElemStatLearn/)) describes the boosting technique for classification trees. Implement the method from scratch.\n",
    "    \n",
    "Write a function `fit_and_score_boosted_trees` satisfying:\n",
    "- Input:\n",
    "    - `x_train`:  Array of predictors in training set\n",
    "    - `y_train`:  Array of binary responses in training set\n",
    "    - `x_test`:  Array of predictors in training set\n",
    "    - `y_test`:  Array of binary responses in training set\n",
    "    - `M`:  Number of iterations / Number of decision trees in the ensemble\n",
    "    - `depth`:  Depth of each decision tree\n",
    "- Fits an ensemble of `T` decision trees to the training set\n",
    "- Output:\n",
    "    - `test_accuracy`:  classification accuracy of the ensemble on the test set\n",
    "\n",
    "Your function will also have to **standardise** the predictors in the training and test sets before applying boosting.\n",
    "   \n",
    "**Hints:** \n",
    "- `sklearn`'s decision tree learning routine has an option to specific weights on the training points\n",
    "- `sklearn`'s classifiers make predictions in {0,1} while the book assumes predictions in {-1, 1}\n",
    "\n",
    "Your implementation will be evaluated based on three test cases: \n",
    "\n",
    "`challenge_testcase_1_train.txt`, `challenge_testcase_1_test.txt`\n",
    "\n",
    "`challenge_testcase_2_train.txt`, `challenge_testcase_2_test.txt`\n",
    "\n",
    "`challenge_testcase_3_train.txt`, `challenge_testcase_3_test.txt`\n",
    "\n",
    "These cases represent extreme examples of data (each dataset contains a particular type of pathology) that might break an implementaiton that is not carefully thought through. \n",
    "\n",
    "**Run the code given below to test your implementation. Call `test_implementation` and pass it your function `fit_and_score_boosted_trees`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  test_implementation\n",
    "# A function that tests your fit_and_score_boosted_trees function using three test sets.\n",
    "# Input: \n",
    "#      fit_and_score_boosted_trees (your implementation of the boosting function)\n",
    "# Returns: \n",
    "#      None\n",
    "\n",
    "def test_implementation(fit_and_score_boosted_trees):\n",
    "    \n",
    "    # Iterate over test cases\n",
    "    for i in range(1,4):\n",
    "        # Load train & test data\n",
    "        data_train = np.loadtxt('testcases/challenge_testcase_' + str(i) + '_train.txt', delimiter=',')\n",
    "        data_test = np.loadtxt('testcases/challenge_testcase_' + str(i) + '_test.txt', delimiter=',')\n",
    "\n",
    "        # Split label and instances\n",
    "        y_train = data_train[:,-1]\n",
    "        x_train = data_train[:,0:-1]\n",
    "\n",
    "        y_test = data_test[:,-1]\n",
    "        x_test = data_test[:,0:-1]\n",
    "\n",
    "        # Run boosting function\n",
    "        print 'Test case', i, ':', fit_and_score_boosted_trees(x_train, y_train, x_test, y_test, 10, 2)    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
