{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Homework 3\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, September 28th, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW3.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farmer, Rick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 109a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sean Keery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Multiple linear regression\n",
    "\n",
    "### Part (a): Implement multiple linear regression from scratch\n",
    "\n",
    "You are provided a data set containing attributes related to automobiles as well as their corresponding prices. The task is to build a linear regression model from scratch that can estimate the price of an automobile (response variable) using its attributes (predictor variables).\n",
    "\n",
    "The file ``dataset_1_train.txt`` contains the training set that you can use to fit a regression model, and the file ``dataset_1_test.txt`` contains the test set that you can use to evaluate the model. In each file, the first two columns contain the predictors of the automobile, namely ``'horsepower'`` and ``'highway MPG'``, and the last column contains the automobile prices.\n",
    "\n",
    "- Implement the following two functions from scratch. \n",
    "\n",
    "    - ``multiple_linear_regression_fit``:\n",
    "\n",
    "        - takes as input: the training set, ``x_train``, ``y_train``\n",
    "\n",
    "        - fits a multiple linear regression model\n",
    "\n",
    "        - returns the model parameters (coefficients on the predictors, as an array, and the intercept, as a float).\n",
    "\n",
    "    - ``multiple_linear_regression_score``:\n",
    "\n",
    "        - takes model parameters (coefficients and intercept) and the test set, ``x_test`` ``y_test``, as inputs\n",
    "\n",
    "        - returns the R^2 score for the model on the test set, along with the predicted y-values.\n",
    "        \n",
    "- Use your functions to predict automobile prices and evaluate your predictions.\n",
    "\n",
    "**Note:** You **may not** use pre-built models or model evaluators for these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Functions for fitting and evaluating multiple linear regression\n",
    "\n",
    "#--------  multiple_linear_regression_fit\n",
    "# A function for fitting a multiple linear regression\n",
    "# Fitted model: f(x) = x.w + c\n",
    "# Input: \n",
    "#      x_train (n x d array of predictors in training data)\n",
    "#      y_train (n x 1 array of response variable vals in training data)\n",
    "# Return: \n",
    "#      w (d x 1 array of coefficients) \n",
    "#      c (float representing intercept)\n",
    "\n",
    "def multiple_linear_regression_fit(x_train, y_train):\n",
    "    \n",
    "    # Append a column of one's to x\n",
    "    n = x_train.shape[0]\n",
    "    ones_col = np.ones((n, 1))\n",
    "    x_train = np.concatenate((x_train, ones_col), axis=1)\n",
    "    \n",
    "    # Compute transpose of x\n",
    "    x_transpose = np.transpose(x_train)\n",
    "    \n",
    "    # Compute coefficients: w = inv(x^T * x) x^T * y\n",
    "    # Compute intermediate term: inv(x^T * x)\n",
    "    # Note: We have to take pseudo-inverse (pinv), just in case x^T * x is not invertible \n",
    "    x_t_x_inv = np.linalg.pinv(np.dot(x_transpose, x_train))\n",
    "    \n",
    "    # Compute w: inter_term * x^T * y \n",
    "    w = np.dot(np.dot(x_t_x_inv, x_transpose), y_train)\n",
    "    \n",
    "    # Obtain intercept: 'c' (last index)\n",
    "    c = w[-1]\n",
    "    \n",
    "    return w[:-1], c\n",
    "\n",
    "#--------  multiple_linear_regression_score\n",
    "# A function for evaluating R^2 score and MSE \n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      w (d x 1 array of coefficients)\n",
    "#      c (float representing intercept)\n",
    "#      x_test (n x d array of predictors in testing data)\n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      r_squared (float) \n",
    "#      y_pred (n x 1 array of predicted y-vals)\n",
    "\n",
    "def multiple_linear_regression_score(w, c, x_test, y_test):        \n",
    "    # Compute predicted labels\n",
    "    y_pred = np.dot(x_test, w) + c\n",
    "    \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # sq_error = \\sum_i (y[i] - y_pred[i])^2\n",
    "    sq_error = np.sum(np.square(y_test - y_pred))\n",
    "    \n",
    "    # Evaluate squared error for a predicting the mean value, against target labels\n",
    "    # variance = \\sum_i (y[i] - y_mean)^2\n",
    "    y_mean = np.mean(y_test)\n",
    "    y_variance = np.sum(np.square(y_test - y_mean))\n",
    "    \n",
    "    # Evaluate R^2 score value\n",
    "    r_squared = 1 - sq_error / y_variance\n",
    "\n",
    "    return r_squared, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score on test set: 0.177944627327\n"
     ]
    }
   ],
   "source": [
    "# Load train and test data sets\n",
    "data_train = np.loadtxt('datasets/dataset_1_train.txt', delimiter=',', skiprows=1)\n",
    "data_test = np.loadtxt('datasets/dataset_1_test.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "# Split predictors from response\n",
    "# Training\n",
    "y_train = data_train[:, -1]\n",
    "x_train = data_train[:, :-1]\n",
    "\n",
    "# Testing\n",
    "y_test = data_test[:, -1]\n",
    "x_test = data_test[:, :-1]\n",
    "\n",
    "# Fit multiple linear regression model\n",
    "w, c = multiple_linear_regression_fit(x_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "r_squared, _ = multiple_linear_regression_score(w, c, x_test, y_test)\n",
    "\n",
    "print 'R^2 score on test set:', r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 0.  0.]]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "def change_x(x):\n",
    "    x[0, 0] = 1\n",
    "    \n",
    "def change_y(y):\n",
    "    y[0] = 1\n",
    "\n",
    "x = np.zeros((2, 2))\n",
    "change_x(x)\n",
    "print x\n",
    "\n",
    "y = [0, 0]\n",
    "change_y(y)\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Confidence interval on regression parameters\n",
    "Using your linear regression implementation from Part (a), model the data in ``dataset_2.txt``, which contains five predictor variables in the first five columns, and the response variable in the last column.\n",
    "\n",
    "Compute confidence intervals for the model parameters you obtain:\n",
    "\n",
    "- Create 200 random subsamples of the data set of size 100, and use your function to fit a multiple linear regression model to each subsample. \n",
    "\n",
    "- For each coefficient on the predictor variables: plot a histogram of the values obtained across the subsamples, and calculate the confidence interval for the coefficients at a confidence level of 95%. \n",
    "\n",
    "- Highlight the mean coeffcient values and the end points of the confidence intervals using vertical lines on the histogram plot. How large is the spread of the coefficient values in the histograms, and how tight are the confidence intervals?\n",
    "\n",
    "- Use the formula for computing confidence intervals provided in class (or use ``statmodels``) to compute the the confidence intervals. Compare confidence intervals you find through simulation to the ones given by the formula (or ``statmodels``), are your results what you would expect?\n",
    "\n",
    "**Note:** You **may not** use pre-built models or model evaluators for these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with all predictors\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJIAAADhCAYAAACTFJ9jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcZGV97/HPbxZZZXFhkXEYQUFxQxKJRg0tSsQkKmrc\nF0ZujDfmuiSvJKAxOEO8UTQ34hI1JirEJWrcwCiCiI3iEkAGZBOJOAwIDIIOy7AKv/vHOQ01NV3d\np7rrVD01/Xm/XvXqqrM+p+p86zz19HPOicxEkiRJkiRJms2iURdAkiRJkiRJ48GGJEmSJEmSJDVi\nQ5IkSZIkSZIasSFJkiRJkiRJjdiQJEmSJEmSpEZsSJIkSZIkSVIjNiS1JCLuiYibI+LvR12WhSgi\nPhERt0bEulGXRaNnHssSEc+oP4+7I+LgUZdHo2M2y2I2NRsz2y4zqNmYwbIs5MzakNSeBB6XmX83\nNSAi9o+IcyJiY0ScHRGP7zVzRLwnIn4aETdGxMUR8aqu8QdHxI/q8f8TEa/tGPfoiPhGRPwyIu7u\nmu9+EfFvEbG2nvfciDi0nw2LiGMj4vp6+e+aZdo/iYjLIuKmiPh6ROzeMW4iIk6PiA0Rcfk08z4+\nIr5Tj18XEW/rGPeWOrQ31Y9bI+I3EfEAgMx8DfDsfrZLW7TN8titzsbH61xcHRF/McO0u0XEiRHx\ni/qAvrxr/EMi4isRcUO9776uY9wj6nHX1Tk6OSL26Zr/HRFxVUT8us7Ifk02MiKWRsR/RsTP63L9\n3izT7xwRX46IW+p5XtY1vmd+6/EHRMQZdRaviYg3dIw7vd7GDRGxJiKeOzUuM7+VmfcHrmiyXdqi\nDTSbXfN9vM7BXh3Djq0zeWO9zx/VMW7GbEbEq+tj+I31Mo6NiMb1qKgqm5fUeftW9/dG17R7RsTX\nIuJX9TZ/oHNdTZZVfx9cEtP8QyUi3hQRl9fzXxQRDwezqUbmVb/tmGfnqOqR3+kY1loG53B8/GR9\nXNsQET+JiP/VY7qj6+Ud3DFsx4g4PiLWR8S1EfH2rnmOiYgfR8RdEXF05zgzqAYGXaf9g4j4bl3n\nvDoiPhoR23eMvzDu+711U73fntgxfqpha+p32Ue7yvHeqOrLN0TEByNicZONnENmZ6zTdkw3XWbv\nFxEfqfN6fVR1/Id0jF8b1W/NqffgG1PjFnJmbUhqT9SP6kXEUuArwL8DO9V/T4yIJT3mvwX4w8zc\nEVgJvC8inlQvawnwJeDD9fiXAv8UEY+t570L+BxwxDTLXQKsA55Wz/t3wOdnqtBuslHVD+LnAo8F\nHgc8JyL+tMe0E8D/BZ4DPABYC/xHxyQbgY8Bf9VjdZ8BJjNzJ2ACeH1E/BFAZr4zM++fmTtk5g7A\nsfW0v2qyHVpwNsljD6uBvYGHAgcDfxMRv99j2nuAk4EXUB3Qu30K+BnwYOCPgH+IiIPqcTsBJwL7\nALsCZ9evq4JGvJgq80+hys0PgU/OUvZO3wVeAVzTYNoPAbfX5Xwl8OGIeFRdjglmyG9EPJDqPfgw\nsDPwcODUjmW/Cdijzu/rgE9FxK5d65/tM9GWb9DZrBYa8RRgLzbP58eA/erj3+8Cr4yIw+pxM2YT\n2IZqv34g8DvAM+h9/OouzwOBLwJ/S5WnH1Edp3v5EHBdXY79gYOA1/e5rL8B1k9Tlj8BXgM8OzO3\np/qOur57sibbpQVpvvXbKccCF3UNay2DtX6Oj+8EHlYfw54LvCMintA5QVSN1H8MXN0173F1WZfX\n5XxVRBzeMf4y4K+B/5ph/WZQvQz6uLkD8PfA7sCjgGXAu6dGZuZjpn5v1b+5rgQ+3zH/VMPW1O+y\nzt+FbwEOAPajyvVvAW+juYHUaafMkNk3U2X1McBDgA3ABzrGJ9Xv8qn3YbpOGAsvs5npo4UH1Q/N\nvTpeHwJc2TXNFcDvN1zeicBf1M93Ae4Gtu4Yfxbwkq559gbubrDs84HnNyzH94A/6Xj9GuD7PaZ9\nD/DBjte71+/Lw7qmewZw+TTz3wI8suP154Eje6zrZ8Aru4YdBKwb9b7gY/SP7jz2mOYXwDM6Xq8G\nPjPLPIvrZS/vGLZdPeyBHcP+BTihxzJ2rqffuX79N8BnO8bvB9w6h22+Evi9GcZvC9wB7N0x7ATg\nH+rnM+aXqpFp2m2aZl0HArcCv901/OfAwaPeP3yM7tFGNutcnktVIey5fGAP4MfAX/UYv0k2pxn/\nF8CJDbfztcCZHa+3rTOxT4/pLwIO7Xj9bqp/HjVaFvCwehnP6jwOUlV01wFPn6W8ZtPHtI/uTDGH\n+i1VI+73gMOB78ww3cAy2DXfjMfHaabfl+qH5x93DT8ZOLQ7L8Avgd/qeP0W4IxplvtJ4Oge6zSD\nPqZ9tHHc7Jr3+cD5PcYdBNwIbNNVnr17TH92Z26AlwFXzGGb51Wn7RjWK7MfAt7V8foPgEs6Xs+a\nx4WYWXskDc+jqSqsnc6vh88oIrYBnkj9n5vMvI6qZ8AREbEoIp5M9V+PM/stVN1D4BFs/l+hXh5d\nl3tKo22oTe1vj2k4/XHA4RGxJCL2BZ4EfLN7orqr44OpemlJfYuInagaSjoz2s++vcniqP5zEV3D\neu33BwHXZOav69efBfauu/gvpeqddPIcyjGbfYC7MvNnHcNm2ubu/D4J+HVEfK/uvn9iRDy0c4aI\n+GpE3EbVq+rbmXnOAMuvBWCO2fxLqh6qF/ZY5pERcTNVxXRbqt6v0+nOZrffY47Hzsy8Ffgfem/H\nccBLI2KbiNiD6lTtqe+BJst6P9WP19u7lrusfjw2qlODfhYRqxpugzSdvuq3UZ2K9gHg/zRY9iAz\n2LeI+OeI2AhcQtWQ9PWOcS8Cbs/Mb/SaveP5IprXfaV5GUCd9iB65+rVwBcz87au4WfUp8V9ISL2\nnGHZi4BlEXH/hmVpatY67SyZ/Rjw1IjYPSK2peoF9fWuaT5d13e/ERGPG3D5x5INScOzPVULbqeb\ngCZB+giwJjM7Txv5LHA0VevrGcDfZuYv+ilQ3e34U8DxmfnThrN1b8dN9bDpfAN4UUQ8pm4MO5qq\n1Xrbhuv6GlX3w9uAi4GPZea500z3auALdWVamovtqRp/uvftvg90mXkL1X9a/y4itoqIA4AXMs1+\nHxHLgA9S/Vd1yjX1/JdSnf75QqofxoO2PdU2durc5tnyu4wqe2+g6jq9lk1PXSUzn1Ov59lM0wgs\nNdBXNuvGzNdS7a/Tysxjs7qewROoegR0H5t7ZbNz/BFUXfT/sdFW9F8H+C7VD8+bqHoQnZ2ZJzVZ\nVkQ8H1jUMX2nZfXfQ6gq2AcDL4se13+RGuh3334j8IPMXDPTQlvIYN8y88+ptu+pVP+svKNe9/ZU\nvXLf2GPWbwBHRsT2UV1/7DU0r/tK8zXnOm1EHAK8iurSJ93jtqH6XfaJrlG/B6wAHklVh/2vuO/a\nZd8A3hQRD4qI3ajqjDD4PMxYp60brmbK7GVU/1z6BdVpbY+kOt1vysuptnFPYBI4JSJ2GEzRx5cN\nScNzC9U5qJ12BG6eaaaIeA/VqS0v6Ri2L9X1EF6ZmUupKoNHRkTji0tHRFA1It3BfaFuons7dqyH\nbSYzvwWsojr4Xl4/bgaualC+nam+fFYBW1H9UD00Iv5313TbAC8Cju9jG7TARcSH476LAh5FtQ8H\nm+/bM+ZzBq+guj7LOuCfqX6sbrLfR8SDgVOoTh/rPNf87VQ9EPcAtgaOAb4dEVvPsSy9zPid1CC/\ntwFfzsxzM/NOqm7Tv9v9X6bMvDszTwGeNXWNM6mXAWTzvcAxdYPujDLzfKoeO8d0laFXNqfGH0ZV\nIT00m1+Xr3EdoD4+fwP4AlVl+0HAA+K+m1v0XFb9n9Rjua+y3H3Nhqn/Ih+bmTdn5hVUp97+QcPt\nkLr1s2/vTrVvTl0jZdprirSUwTnJyvep6qF/Vg9eDfx7Zl7ZY7Y3UNWvLwO+TNXrcda6rzQXg6rT\n1tfi/TTwwq6ePVNeCNyQmd/tHJiZZ2bmbzLzJqprmK2gutYSVDldA5xHdebMl6l6Dm12/b55mu17\naBUzZ/ZDVL83d6a6RMWXqY7DAGTmDzLzjsy8PTPfRdXY9LTBFX882ZA0PBdRXZy60+OYoUtuRKym\nur7BIV2V4scAP8nM0wAy8zKq3jv93KXsY1SV0xdk5t2zTdzhIqDzbhz7M8M2ZOaHM3OfzNyd6gfp\nEmDa0w267AX8JjM/nZn3ZObVVL2wuiu7L6D6UvvOZkuQesjMP8v7Lgr4rszcQNVtvXPffjxz7DKf\nmVdm5nMyc9fMfDLVqZdnTY2vux2fAnylPiB1ejzVNZKuqff9E6gObI3u3NaHnwJLImLvrnXfu82z\n5PfHbH4h4+kuPD5lCdV126SeBpDNZwDviepuS1MX5/xBRLy0x/RLqI43wKzZJKq7nP4L8EeZeXEf\nm3YR1fFyajnbUeVhuu14ANWP1n/OzLvq03o+wX3Hv5mW9Qiq/5h+t97+LwIPqU85WE7V0/HOrvXN\nlFtpNv3Ubw8EdgMurvfP44DfqffPgFYzOF+dx7CDgTd2fM88lOrGNX8NkJkbMvOVmbl7Zj6W6rpt\nZ027VGmeBlGnrS8k/xVgZWZO9pjs1VQX059JdP6tG17emJnLMvPhwK+pbhAxaLPVaWfMbD3tJzLz\nxsy8i+r02wOjvhv4NLovYbEwzeXCSj5mf7D5xQiXUl2E6w3A/aj+I/NzYEmP+d9CFYpdphm3F1V3\nxafXr/em+q/H/+qYZiuqH5731M/v1zHuI8D3gW1nKPu0FzSjuvvSRVRXtN+jfv7aHtNuBTy6fr4c\n+Dbw9x3jo57m2VSnxWwFLK3H3R/4FdUd6YKq4vH9zvnr6U4BVvVYvxfb9jG1LzS5MOE76310J6r/\npFxD1Yjba/qtuO/C2vsAW3WMeyRVN9ulVHeOuI764tv1vn0W8P4eyz0a+A7VRfWDqovxzcAO9fhP\nAB+foVz3o+rJdCXV6StbzTDtZ6j++7QtVdf9XwOP6ti+mfL7dOAGqh8MS6l6gpxRj9uX6mKGW1NV\nvl9J1fNj/671L7gLE/rY9DHobFL9g2SX+rFrvfwn1vtzAH8K7FRPeyBVZfvP69ezZfNgqrubPbXH\n+J7ZrMv1a6qLmG5FdfHsaW9UUU//P1R3dVpcb/eXgE/Ntqx6+l06Hs+n6gnxYCDqaY4HTqq/o5ZR\nXf9lZdf6zaaPaR/dmaWP+m09bef++UbgB8CD6/GtZbAe3+j4WOflJVTH+EVU/9S9mequTVD9c6dz\nO9ZR/WNz23r8XlQNwouo6rjXsenNY5bU5fg01ekzW1GdjtpZBjPoY9pHdwZ7TNPPcfMxwLXAi2ZY\n3jKqu4J33zBpP6pGmEX1MeV99TFlcT3+IcDu9fMn1VnpvAj4sOq0s2X248B/UvVqWgq8lfomAlSN\nTr9bD9+K6ti8nq6bACzEzI68AFvqY7qQ10E7h+q6J+dQ3SpxatzLgQu65r+N6vzOm+u/R3WM/2Pg\nAqoGpXV0XJWe6r+R91Dd2e3u+vnl9bjl9etb6+VOLftl9fiHUnXXm/YOGfU076L6AXk98M6ucRd2\nLGtHqgud3UxVWX8HdUW2Hn9QVznvBk7vGD9BVaH4dT3/R9j0TnUPofrPaq878kxgQ5KPbHzQvR9V\nT70bqQ64b+oafzPwlK5ldmbs7o5xb6KqON5M1Sj0hI5xr67nuZlNM7isHr8V1X9Crq6zeA4dB3/g\nNOCIGbbj512Zupv6rnJUDdRf65h2Z6ruu7dQNea+pGPcjPmtp3kd1Y/UG6juLLlHPfyRVBfYvpGq\nQfi/gef2KOuCOuj62GwfGHg2u8bdPbV8qoakk6mOXTcBP6HjTqANsnl6fczpPC535mm2bB5MVcHe\nWC+r826P3dl8HNWPgF/V3yWfpf6xPduyuta52T9UqH6s/0dd/iuorrHYPZ/Z9DHtY7rM0kf9tmu+\nw+m4a9sQMtjo+EjVWDtZ528D1bFwpuVezqZ3gHoR1bVWbqG6g+Qzu6b/BJvXf189TVnNoI/NHtNl\ncJppGh83qRpRftORq5u7MwscRXUTi+71PJ3qWHozVWPUl9j0zmlPq/flW+pj1ku75h9KnXaa5XZn\n9gFUl3xZX+f+O9R3GqZqLJuqD/+S6pqfT+hR1gWV2an/TrUmInYE/o37bsN7BFVPm89RNXisBV6c\nmZtd7HKcRcStVOdHvz8z3z7q8jQVEa8A9svMvx11WeYjIv6N6kB+bWbuO+rylGghZXNc89itvovb\neVSV9H5OSS1KRBxMdcrNUqr/8J4x4iIVxWyOH7O5MCykbPZSambN4MK2kLJZagb7ZWbH3zAako6n\nOt3hE/Vdwraj6i52Q2a+OyKOpOr9clSrBZG0CbMplclsSmUym1KZzKY0fK02JNW3xVuTmXt3Df8J\ncFBmrq9vBTiZmY9srSCSNmE2pTKZTalMZlMqk9mURqPtu7Y9DLg+Ij4REedGxEfrW9PumvVt/zLz\nWqqLXkkaHrMplclsSmUym1KZzKY0AkuGsPwDqO6Ick5EvJfqYl3d3aCm7RYVEe2edycVLjPburWk\n2ZTmwWxKZTKbUpnMplSmuWaz7R5JV1HdOu+c+vUXqYK+PiJ2Bai7Gl7XawF9X0F8LvP0eLz97W8f\n6ZXQe62/+h6sHqNY/5wec/hcSn3/h/Vo2fCz2eK+Uspn1qRsw8rvwN6zNj/rMf08W1ZuNgf9WRZS\n1kHuZ4PMd+v7/zzef7MJFJDNez+HAS67rc92rsudKVOllXWmz6KNsha5/T0/x1YVl81iPoeWtq2d\nfWT+x89Sj02blaugfW4+Wm1Iyqo74ZURsU896BnARcBJwMp62OFUt42WNCRmUyqT2ZTKZDalMplN\naTTaPrUN4I3Ap+tb/F0OvAZYDHw+Io4ArgBePIRySNqU2ZTKZDalMplNqUxmUxqy1huSMvN84InT\njHpm2+uer4mJCdfv+rdY45zNXkr+zEotW6nlgrLL1iazOVyllq3UckHZZWtTadls43No67O1rOOx\nzDaX26bSsjkIJX8OpZbNcg1XzPfcuDZFRPZdvggoeJsGITouhzU2m7oAPpdBiwiyvQsTzsucstl8\n4Vv0vjJ2+d3CP4+5WLDZHLQtcN8aq3xvke//As3mFvhZThmrTMEW/VnMx4LN5qiN0f44dlmfj4I+\nl/lks+2LbUuSJEmSJGkLYUOSJEmSJEmSGrEhSZIkSZIkSY0M465tqh199HGsW7dhXstYvnwn4M2D\nKZAkSQLmf4z2+CxJ0mAM6nfzMcd4XG6LDUlDtG7dBlasWDWvZaxdO7/5JUnS5uZ7jPb4LEnSYPi7\nuXye2iZJkiRJkqRG7JEkSQ0M4rQXu9dKkiRJGnc2JElSA572IkmSJEk2JEmSJEkaIG8wI0lbNhuS\nJEmSJA2MF8qVpC2bF9uWJEmSJElSI/ZIGjNr1py/yeuVK1f1vQwv+itJkqSSzbfOa31XktpjQ9KY\n2bgxN3k9l27DdhWWJElSyeZb57W+K0nt8dQ2SZIkSZIkNWKPJEmSJEmSNG9zu2vjqnufrVy5ijVr\nLmbFikGWSoNmQ5IkSZIkSZq3+d61ccWKVZx55mGDK5Ba4altkiRJkiRJasQeSZI0BGvWnF/fcWbV\nvcP6uQONd5+RyuYdpiRJ0kLRekNSRKwFbgTuAe7KzAMjYmfgc8CewFrgxZl5Y9tlkXQfszlcGzfm\nZt18++n2691nFg6zOZ68w9SWz2xKZTKb0vAN49S2e4CJzHxCZh5YDzsKOC0z9wVOB94yhHJI2pTZ\nlMpkNqUymU2pTGZTGrJhNCTFNOt5HnBC/fwEwKtpScNnNqUymU2pTGZTKpPZlIZsGA1JCXwzIs6O\niD+ph+2amesBMvNaYJchlEPSpsymVCazKZXJbEplMpvSkA3jYttPycxrIuLBwKkRcSlV2Dt1v77X\nqlWr7n0+MTHBxMREG2WURm5ycpLJyclhrtJsSg2YTalMZlMqk9mUyjTIbLbekJSZ19R/fxkRXwEO\nBNZHxK6ZuT4idgOu6zV/Z7ClLVn3gWv16tWtrs9sSs2YTalMZlMqk9mUyjTIbLZ6altEbBsR29fP\ntwN+H7gAOAlYWU92OHBim+WQtCmzKZXJbEplMptSmcymNBpt90jaFfhyRGS9rk9n5qkRcQ7w+Yg4\nArgCeHHL5ZC0KbMplclsSmUym1KZzKY0Aq02JGXmz4H9pxn+K+CZba5bUm9mUyqT2ZTKZDalMplN\naTSGcdc2SZIkSZIkbQFsSJIkSZIkSVIjNiRJkiRJkiSpERuSJEmSJEmS1IgNSZIkSZIkSWrEhiRJ\nkiRJkiQ1YkOSJEmSJEmSGrEhSZIkSZIkSY3YkCRJkiRJkqRGbEiSJEmSJElSIzYkSZIkSZIkqREb\nkiRJkiRJktSIDUmSJEmSJElqxIYkSZIkSZIkNWJDkiRJkiRJkhqxIUmSJEmSJEmN2JAkSZIkSZKk\nRmxIkiRJkiRJUiM2JEmSJEmSJKmRoTQkRcSiiDg3Ik6qX+8cEadGxKURcUpE7DiMckjalNmUymMu\npTKZTalMZlMavmH1SHoTcHHH66OA0zJzX+B04C1DKoekTZlNqTzmUiqT2ZTKZDalIWu9ISkilgF/\nAPxbx+DnASfUz08ADmu7HJI2ZTal8phLqUxmUyqT2ZRGYxg9kt4L/DWQHcN2zcz1AJl5LbDLEMoh\naVNmUyqPuZTKZDalMplNaQSWtLnwiPhDYH1mnhcREzNMmr1GrFq16t7nExMTTEzMtBhpfE1OTjI5\nOTmUdZlNqblhZXMQuQSzqYXDbEplMptSmQaZzVYbkoCnAM+NiD8AtgHuHxGfBK6NiF0zc31E7AZc\n12sBncGWtmTdB67Vq1e3uTqzKTU0xGzOO5dgNrVwmE2pTGZTKtMgs9nqqW2Z+dbMXJ6ZewEvBU7P\nzFcBXwVW1pMdDpzYZjkkbcpsSuUxl1KZzKZUJrMpjc6w7trW7V3AIRFxKfCM+rWk0TObUnnMpVQm\nsymVyWxKLWt0altEPDYzL5jPijLzDOCM+vmvgGfOZ3mSzKZUqvlm01xK7TCbUpnMpjRemvZI+lBE\nnBURr4+IHVstkaR+mE2pTGZTKpPZlMpkNqUx0qghKTOfBrwCeCjwo4j4TEQc0mrJJM3KbEplMptS\nmcymVCazKY2XxtdIyszLgLcBRwIHAe+PiJ9ExAvaKpyk2ZlNqUxmUyqT2ZTKZDal8dGoISkiHhcR\n7wUuAQ4GnpOZj6qfv7fF8kmagdmUymQ2pTKZTalMZlMaL40utg18APg34K2ZedvUwMy8OiLe1krJ\nJDVhNqUymU2pTGZTKpPZlMZI04akPwRuy8y7ASJiEbB1Zt6amZ9srXSSZmM2pTKZTalMZlMqk9mU\nxkjTaySdBmzT8Xrbepik0TKbUpnMplQmsymVyWxKY6RpQ9LWmXnL1Iv6+bbtFElSH8ymVCazKZXJ\nbEplMpvSGGnakLQxIg6YehERvwXcNsP0kobDbEplMptSmcymVCazKY2RptdIejPwnxFxNRDAbsBL\nWiuVpKbMplQmsymVyWxKZTKb0hhp1JCUmWdHxCOBfetBl2bmXe0VS1ITZlMqk9mUymQ2pTKZTWm8\nNO2RBPBEYEU9zwERQWb+eyulktQPsymVyWxKZTKbUpnMpjQmGjUkRcQngb2B84C768EJGGxphMym\nVCazKZXJbEplMpvSeGnaI+m3gf0yM9ssjKS+mU2pTGZTKpPZlMpkNqUx0vSubRdSXfBMUlnMplQm\nsymVyWxKZTKb0hhp2iPpQcDFEXEWcMfUwMx8biulktSU2ZTKZDalMplNqUxmUxojTRuSVrVZCElz\ntmrUBZA0rVWjLoCkaa0adQEkTWvVqAsgqblGDUmZeUZE7Ak8IjNPi4htgcXtFk3SbMymVCazKZXJ\nbEplMpvSeGl0jaSIeC3wBeBf6kF7AF9pq1CSmjGbUpnMplQmsymVyWxK46Xpxbb/HHgKcBNAZl4G\n7DLbTBGxVUT8d0SsiYgLIuLt9fCdI+LUiLg0Ik6JiB3nugHSAmc2pTKZTalMZlMqk9mUxkjTayTd\nkZl3RgQAEbEEmPXWjJl5R0Q8PTNvjYjFwPci4mTghcBpmfnuiDgSeAtw1Nw2QVrQzKZUJrOpvqxZ\ncz4rV66a07zLl+/EMce8ebAF2nKZTalMZlMaI00bks6IiLcC20TEIcDrga82mTEzb62fblWvL4Hn\nAQfVw08AJjHY0lyYTalMZlN92bgxWbFi1ZzmXbt2bvMtUGZTKpPZlMZI01PbjgJ+CVwAvA74OvC2\nJjNGxKKIWANcC3wzM88Gds3M9QCZeS0Nui1KmpbZlMpkNqUymU2pTGZTGiNN79p2D/Cv9aMv9bxP\niIgdgC9HxKPZvJtiz26Lq1atuvf5xMQEExMT/RZBGguTk5NMTk72NY/ZXDjmc9oLeOrLfJhNqUxm\nUzOZ73ETPHbOldmUyjSXbPbSqCEpIn7ONOHLzL2arigzb4qISeBQYH1E7JqZ6yNiN+C6XvN1Blva\nknUfuFavXj3rPGZz4ZjPaS/gqS/zYTalMplNzWS+x03w2DlXZlMq01yy2UvTayT9dsfzrYEXAQ+Y\nbaaIeBBwV2beGBHbAIcA7wJOAlYCxwKHAyf2UWZJ9zGbUpnMplQmsymVyWxKY6TpqW03dA06LiJ+\nBBw9y6y7AydExCKq6zF9LjO/HhE/BD4fEUcAVwAv7rPckjCbUqnMplQmsymVyWyW4eijj2Pdug3z\nWsaaNRezYsVgyqNyNT217YCOl4uoWoxnnTczLwAOmGb4r4BnNiyjpB7MplQmsymVyWxKZTKbZVi3\nbsO8Twk988zDBlMYFa3pqW3/r+P5b4C12KorlcBsSmUym1KZzKZUJrMpjZGmp7Y9ve2CaHi8i8WW\nw2xKZTKbUpnMplQmsymNl6antv3lTOMz858GUxwNg3ex2HKYTalMZlMqk9mUymQ2pfHSz13bnkh1\n9XuA5wBnAZe1UShJjZlNqUxmUyqT2ZTKZDalMdK0IWkZcEBm3gwQEauAr2XmK9sqmKRGzKZUJrMp\nlclsSmWLPdIZAAAY6ElEQVQym9IYWdRwul2BOzte31kPkzRaZlMqk9mUymQ2pTKZTWmMNO2R9O/A\nWRHx5fr1YcAJ7RRJUh/MplQmsymVyWxKZTKb0hhpete2/xsRJwNPqwe9JjPXtFcsSU2YTalMZlMq\nk9mUymQ2pfHS9NQ2gG2BmzLzfcBVEfGwlsokqT9mUyqT2ZTKZDalMplNaUw0akiKiLcDRwJvqQct\nBT7VVqEkNWM2pTKZTalMZlMqk9mUxkvTHknPB54LbATIzKuB+7dVKEmNmU2pTGZTKpPZlMpkNqUx\n0rQh6c7MTCABImK79ookqQ9mUyqT2ZTKZDalMplNaYw0bUj6fET8C7BTRLwWOA341/aKJakhsymV\nyWxKZTKbUpnMpjRGmt617R8j4hDgJmBf4OjM/GarJZM0K7MplclsSmUym1KZzKY0XmZtSIqIxcBp\nmfl0wDBLhTCbUpnMplQmsymVyWxK42fWhqTMvDsi7omIHTPzxmEUqkRHH30c69ZtmNcy1qy5mBUr\nBlMeyWyqH2vWnM/Klav6nu94uHe+5ct34phj3jzIYm2RzKZUJrMplclsSuOn0altwC3ABRHxTeor\n6QNk5htbKVWB1q3bwIoVq+a1jDPPPGwwhZHus+CzqWY2bsw5foetvne+tWvnMv+CZTalMplNqUxm\nUxojTRuSvlQ/JJXFbDY0316F9ihUn8ymhmYuPQ6PZ8H2NjSbDXjMrPSbreNhk+kXWLbmy2wOQGd2\nj4e+jw1bSnZh7r3xp5jfmc3YkBQRyzNzXWaeMKwCSZqd2ezffHsV2qNQTZhNjcLcehwurN6GZrM/\nHjMr/Wdr9SbTL4RszZfZHKxNs7u67xxvKdmF+fTGr5jfmS2aZfxXpp5ExBf7XXhELIuI0yPiooi4\nICLeWA/fOSJOjYhLI+KUiNix32VLC5zZlMpkNqUymU2pTGZTGkOzndoWHc/3msPyfwP8ZWaeFxHb\nAz+KiFOB11Bdmf/dEXEk8BbgqDksXyMy366+YHfBeTKbUpnM5oh4Ko5mYTalMplNaQzN1pCUPZ43\nkpnXAtfWz2+JiEuAZcDzgIPqyU4AJjHYY2W+XX3B7oLzZDalMpnNEfFUHM3CbEplMpvSGJqtIenx\nEXETVUvxNvVz6teZmTs0XVFErAD2B34I7JqZ66kWcm1E7NJvwaUFzmxKZTKbUpnMplQmsymNoRkb\nkjJz8SBWUncz/ALwprqluLu1uWfr86pVq+59PjExwcTExCCKJBVncnKSycnJRtOaTWl4zKZUJrMp\nlclsSmXqJ5uzma1H0rxFxBKqUH8yM0+sB6+PiF0zc31E7AZc12v+zmBLW7LuA9fq1atbXZ/ZlJox\nm1KZzKZUJrMplWmQ2Zztrm2D8HHg4sx8X8ewk4CV9fPDgRO7Z5LUOrMplclsSmUym1KZzKY0ZK32\nSIqIpwCvAC6IiDVUXQrfChwLfD4ijgCuAF7cZjkkbcpsSmUym1KZzKZUJrMpjUarDUmZ+T2g13mv\nz2xz3ZJ6M5tSmcymVCazKZXJbEqjMYxT2yRJkiRJkrQFsCFJkiRJkiRJjdiQJEmSJEmSpEZsSJIk\nSZIkSVIjNiRJkiRJkiSpERuSJEmSJEmS1IgNSZIkSZIkSWrEhiRJkiRJkiQ1YkOSJEmSJEmSGrEh\nSZIkSZIkSY3YkCRJkiRJkqRGbEiSJEmSJElSIzYkSZIkSZIkqREbkiRJkiRJktSIDUmSJEmSJElq\nxIYkSZIkSZIkNWJDkiRJkiRJkhqxIUmSJEmSJEmN2JAkSZIkSZKkRlptSIqIj0XE+oj4ccewnSPi\n1Ii4NCJOiYgd2yyDpM2ZTalMZlMqk9mUymQ2pdFou0fSJ4BndQ07CjgtM/cFTgfe0nIZJG3ObEpl\nMptSmcymVCazKY1Aqw1JmXkm8Ouuwc8DTqifnwAc1mYZJG3ObEplMptSmcymVCazKY3GKK6RtEtm\nrgfIzGuBXUZQBkmbM5tSmcymVCazKZXJbEotWzLqAgA508hVq1bd+3xiYoKJiYmWiyONxuTkJJOT\nk6MuRiezKWE2pVKZTalMZlMq0yCzOYqGpPURsWtmro+I3YDrZpq4M9jSlqz7wLV69ephF8FsStMw\nm1KZzKZUJrMplWmQ2RzGqW1RP6acBKysnx8OnDiEMkjanNmUymQ2pTKZTalMZlMaslZ7JEXEZ4AJ\n4IERsQ54O/Au4D8j4gjgCuDFbZZhytFHH8e6dRvmPP+aNRezYsXgyiONUknZbGK++QUzrPEwbtkc\ntPlk3YyrTeOUTY+ZWkjGKZsaL2vWnM/KlavmtYzly3fimGPePJgCFabVhqTMfHmPUc9sc73TWbdu\nAytWrJrz/Gee6cX+teUoKZtNzDe/YIY1HsYtm4M2n6ybcbVpnLLpMVMLyThlU+Nl48ac93fp2rXz\nm79ko7hrmyRJkiRJksaQDUmSJEmSJElqxIYkSZIkSZIkNWJDkiRJkiRJkhqxIUmSJEmSJEmNtHrX\ntkG47LLLuP766xtP/2TgBz/4wSbDdtpppwGXSlJTt99+O+eddx6Z2Wj66TJ8yy23tFAySYN08803\nc+GFFzaevjvrS5cubfw9IW2pfvrTn3LDDTf0NU93lu68884Bl0qSpE0V35D0/vd/iZtuejSLFjUr\n6pOBj370xq6hp1g5lUbkoosu4p/+6Vy2226vRtN3Z/jOO2/lssuu4jGPaamAkgbinHPO4QMf+Bk7\n7ris0fTdWb/rrgu5445bWyqdNB7e974vcsstj2PRosWN5+nM0k03Xc2GDb9kn31aKqAkqbE1a85n\n5cpVmww7HjYbNpPly3fimGPePMhiDUTxDUn33APLlh3M0qXbNp5nzz0P3eT1z3/+w0EXS1Iftt9+\nGcuXHzr7hLXODN9yy3ouuODzbRRL0gBlwg47PJw995xoPE9n1q+66iruuGN9CyWTxsc998BDH/oM\nlizZuq/5prJ05ZXfZ8OGH8wytSRpGDZuTFasWNU1dPU0w3pbu7b5tMPkNZIkSZIkSZLUiA1JkiRJ\nkiRJasSGJEmSJEmSJDViQ5IkSZIkSZIasSFJkiRJkiRJjdiQJEmSJEmSpEZsSJIkSZIkSVIjNiRJ\nkiRJkiSpERuSJEmSJEmS1IgNSZIkSZIkSWrEhiRJkiRJkiQ1MrKGpIg4NCJ+EhE/jYgjR1WOmaxd\nO+n6R2hycmGvf1TGIZu9jHqfncltt10/6iJMq+T3bKFmsBez2Y5Sy1ZqucBsdhtVNtvYR9ra7xZ6\nWdvITFs53JLy3TSbP/rRjzjjjDPm9bj66qsHWvaSP4dSj0+Wa7iWjGKlEbEI+CDwDOBq4OyIODEz\nfzKK8vSydu0kK1ZMuP6WrFlzPitXruo5/rzzJtl//5nXv3z5ThxzzJvnVY6jjz6Odes2zGn9gyxH\nCcYlm72MOjMzKbkhqel7NltmZ9NvTiYnJ5mYmLj3da+strX+kpjN9pRatmGWq99sdx8f55Otcc/1\nKLPZxj7S1n63UMs6la1+6pSdZtq/u4+RvfSbsUHme5T6yeb7338y8LtzXtett/6SZz/7lxxxxB/P\neRmw6Wc1131mzZqLWbFiXsWYlcfN/pRarvkaSUMScCBwWWZeARARnwWeB4xFhViDsXFjsmLFqp7j\n165dNeP4qWnma926DdOup8n6B1mOQphN9TRbZmcz35z0yuqw1j9iZlOt6Tfb3cfH+WRrC8i12VRP\nU9nqp07Zqc167kzrHFS+R6xxNhcvXsry5QfPeUXXXXfhdIvtW+dnNdd95swzD5t3OaQmRtWQtAdw\nZcfrq6jCvplttlnM1Vd/kUWLFjde+JVXfmaT11tvvYjbb797DsWUFpzG2Wxq8eLF3HHHzzbL5Uw6\np/3Nb+5g8eKYTxGkLcHAszloS5Ys5vbbz+fKKzfv3n/jjRdM+x3QOezuu68nwqxr7Aw0m1tvvZir\nr/4CVWeKmXXmaurvrbf+mkWLzJFEH9lcvPjOvuqp3W6//WaWLn3wnOeXxlFk5vBXGvFC4FmZ+af1\n61cCB2bmG7umG37hpIJk5lBrg2ZTasZsSmUym1KZzKZUprlmc1Q9kn4BLO94vawetolhf+FIMptS\nocymVCazKZXJbEotGtVd284GHh4Re0bE/YCXAieNqCyS7mM2pTKZTalMZlMqk9mUWjSSHkmZeXdE\n/B/gVKrGrI9l5iWjKIuk+5hNqUxmUyqT2ZTKZDaldo3kGkmSJEmSJEkaP6M6tW0TEXFoRPwkIn4a\nEUdOM/7lEXF+/TgzIh47zPV3TPfEiLgrIl4w7PVHxERErImICyPi28Ncf0TsEBEnRcR5EXFBRKwc\n4Lo/FhHrI+LHM0zz/oi4rF7//oNad5P1D2Hfm3X76+la2feaioidI+LUiLg0Ik6JiB17TLe2fq/W\nRMRZLZanSWZa22/mWq6IOCgiNkTEufXjbUMq10hzNp+yjfA9WxYRp0fERfX33ht7TDeS961j/X9c\nHxfujogDZpiu0XFugOXyO2NAZfN7o79yjer9qtfdSn22aX77qSs0zERfdc8G2993fbKt/bDBftT3\nZ9WkrPV0/XxOTba/798IDbZ/Lp9V8cfNUo+Z9To9bg6obB43+yvXnN+vzBzpg6ox63+APYGlwHnA\nI7umeRKwY/38UOCHw1x/x3TfAv4LeMGQt39H4CJgj/r1g4a8/rcA75xaN3ADsGRA638qsD/w4x7j\nnw18rX7+O4P87Buuv7V9r8n629z3+iznscDf1M+PBN7VY7rLgZ1bLkuTfbbV/WYe5ToIOGkEn99I\nczbPso3qPdsN2L9+vj1waQn72TTl3Bd4BHA6cECPaRod5wZcLr8zBlc2vzf6K9eo3q9W6rNN80sf\ndYWGZe2r7tlwmX3XJ9vaDxssdy6f1cDrdA3KOaffCA2WO5fPqvjjJoUeM+v1etwcXNk8bvZXrjm9\nXyX0SDoQuCwzr8jMu4DPAs/rnCAzf5iZN9YvfwjsMcz1194AfAG4boDrbrr+lwNfzMxfAGTm9UNe\nfwL3r5/fH7ghM38ziJVn5pnAr2eY5HnAv9fT/jewY0TsOoh1N1l/y/tek+2H9va9fjwPOKF+fgJw\nWI/pgvZ7OjbZZ1vdb+ZRLqjeo6Eadc7mWTYYzXt2bWaeVz+/BbiEzfM/sveto5yXZuZlzPweNd03\nB8nvjMGVDfze6KdcMIL3i/bqs23UU9uoe7ZSn2xrP2yj/tdGna7BMuf0G6HBcufyWRV/3Cz4mAke\nNwdZNvC42U+5YA7vVwkNSXsAV3a8voqZv6z/BDh5mOuPiIcAh2Xmhxn8Ttlk+/cBHhAR346IsyPi\nVUNe/weB/SLiauB84E0DXP9susv3CwbcmNOHQe97s2p53+vHLpm5HqqKArBLj+kS+Ga9n762pbI0\n2WdHsd80/S57ct2d9WsRsV/LZWqqpJxNZ6TvWUSsoPpPzn93jSr9fZvS73F2EPzOaMbvjXaM4v1q\nqz7bRj21jbrnqOqTw9gPB1L/a6lO19ZvhHl9VmN+3BzFMRM8bjblcbMdfb9fI7lr21xFxNOB11B1\nzxqm46i6GN5blCGvfwlwAHAwsB3wg4j4QWb+z5DW/yxgTWYeHBF7U315Pa7+b8OCsBD2vYj4JtDZ\nKh5UB6vpzpPtdZX+p2TmNRHxYKr95JK6FVyVHwHLM/PWiHg28BWqSqB6G+l7FhHbU/33+E2j+s6b\nIZt/m5lfHUWZwO+MIfJ7oz/Fv18t1CnaqCu0Ufccu/rkgD+rcfmcYB6f1aiPm6UeM8Hj5hAVfxwo\nzJzerxIakn4BLO94vawetomIeBzwUeDQzJyta9ag1//bwGcjIqjOE352RNyVmScNaf1XAddn5u3A\n7RHxHeDxVOeHDmP9rwHeCZCZP4uInwOPBM4ZwPqblO+hs5SvVS3ue020ue9tIjMP6TWuvkDbrpm5\nPiJ2o0eX7My8pv77y4j4MlX300Ef3Jrss6PYb2YtV2eFKjNPjogPRcQDMvNXLZdtNiPPWS+jfM8i\nYglVZfiTmXniNJMM5X2bKZsNNTrO9svvjIHwe2PARvh+tVWfbaOe2kbdc1T1ydb2wxbqf23U6dr6\njTCnz6qE42apx0zwuDkgHjcHbK7vVwmntp0NPDwi9oyI+wEvBTb5Qo2I5cAXgVdl5s+Gvf7M3Kt+\nPIzqy/H1A/whP+v6gROBp0bE4ojYluriXJcMcf1XAM8EiOo8zn2oLvQ2KEHv/8qcBLy6XveTgA1T\n3T6Hsf6W971Z19/yvtePk4CV9fPDqfbJTUTEtvV/oYiI7YDfBy5soSxN9tlh7Dd9lys6zoOOiAOB\nGOJBbdQ5m8lMGRzle/Zx4OLMfF+P8aN+37r1+nybZGbQ/M4YUNn83uivXCN8v9qqz7ZRT22j7tlm\nfbKt/bCN+l8bdbqZtn8+vxFmWu5cP6txOm6WdMwEj5sDK5vHzf7KNef3K4d8NfPpHlR3Q7gUuAw4\nqh72OuBP6+f/SnW3gHOBNcBZw1x/17QfZ8B3zmqyfuCvqO7K8GPgDUN+/3cHTqnX/WPgZQNc92eA\nq4E7gHVU/wHp3vYPUv1n5Xx63GGhrfUPYd+bdfvb3Pf6KOcDgNPq/eRUYKeOfeO/6ucPo7pzwhrg\ngql9qaXyNMlMa/vNXMsF/DnVAX8N8H3gd4ZUrpHmbD5lG+F79hTg7o59+tz68y3ifetY/2FU59vf\nBlwDnFwPvzebvfbNlsvld8aAyub3Rn/lGtX71fCznFOdosn+2zFto7pCw0z0VfdssP191yfb2g8b\n7Ed9f1ZNyjqHz6nJ9vf9G6HB9s/lsyr+uEmhx8x6nR43B1Q2PG72Va65vl9RzyxJkiRJkiTNqIRT\n2yRJkiRJkjQGbEiSJEmSJElSIzYkSZIkSZIkqREbkiRJkiRJktSIDUmSJEmSJElqxIYkSZIkSZIk\nNWJD0hiKiLsj4tyIuCAiPhcRW89jWQdFxFfr58+JiL+ZYdodI+LP5rqujuW8IyLWRcRN812WVJJx\nzmZEbBMR/xURl9Tl/4f5LE8qyThns17OyRGxpi7/hyIi5rtMqQTjns2O5Z0UET8e1PKkURr3XEbE\ntyPiJ/Vx89yIeNB8l6nN2ZA0njZm5gGZ+VjgLuB/d0/QZyUzATLzq5n57hmm2xl4fT8FjYjF0ww+\nCXhiP8uRxsS4Z/M9mfko4AnAUyPiWf0sUyrYuGfzRZn5hLr8uwAv6meZUsHGPZtExPMB/zmqLcnY\n5xJ4WX3cPCAzr+9nmWrGhqTx913g4RGxZ93yekJEXAAsi4hDIuL7EXFO3Zq8LUBEHFr3OjgHeMHU\ngiLi8Ij4QP18l4j4UkScV7fmPgl4J7B33bJ7bD3de+rW6vMj4sX1sIMi4jsRcSJwUXeBM/OszFzf\n9hsjjdhYZTMzb8vMM+rnvwHOBZa1/SZJIzBW2QTIzFvq6ZYC96OulEtbmLHLZkRsB/wF8I523xpp\nZMYulzXbOVq2ZNQF0JwEQEQsAZ4NnFwPfwTwqsw8OyIeCLwNeEZm3hZVN8K/jIj3AB8FJjLz8oj4\nXNeypyqn7wcmM/MFERHA9sBRwKMz84B6/S8AHpeZj42IXYCzI+KMev4n1NOua2H7pVJtEdmMiJ2A\n5wDHzeO9kEoy9tmMiG9Q9eY9GfjC/N4OqRjjns2/B/4RuG2+b4RUkHHPJcDxEXEX8KXMtKG3BbbU\njadtIuJc4CzgCuBj9fC1mXl2/fxJwH7A9yJiDfBqYE/gkcDlmXl5Pd2neqzjYODDAFm5eZppngr8\nRz3NdcAk952ydpaNSFqAxj6bUXUR/gxwXGaunXFrpfEx9tnMzEOB3YGt6nVJW4KxzWZEPB7YOzNP\novrh7bXLtKUY21zWXl6flvc04GkR8cpZtldzYI+k8XTrVEvtlKohl42dg4BTM/MVXdM9nmYHurl0\nm+9c7saeU0lbri0hmx8FLs3MD8xhPVKptoRskpl3RsRJwPOAb81hfVJpxjmbTwZ+KyIuB5YCu0TE\n6ZlpQ6/G3Tjnksy8pv67MSI+AxxI7wYtzZE9ksZTr3B2Dv8h8JSI2BsgIraNiEcAPwH2jIiH1dO9\nrMeyvkV9sbOIWBQROwA3A/fvmOa7wEvq8Q+mavU9awDbIY2rsc5mRLwD2CEz/2K2aaUxM7bZjIjt\nImK3+vkS4A/rMklbgrHNZmZ+JDOXZeZeVD0nLrURSVuIsc1lRCyuT7ubuq7gHwEXzjSP5saGpPHU\nqwX33uH11elXAv8REecD3wf2zcw7gNcBX4/qAmi9Lnr9ZuDpUd3K9BzgUZn5K+D7EfHjiDg2M78M\nXACcD5wG/HXd7XBGEXFsRFxJ1W1yXUQc3WCbpXEwttmMiD2AtwL7xX23Sz2i2WZLxRvbbALbASdF\nxHlUF8FfD3xk1i2WxsM4Z1PaUo1zLrcCTuk4Zl4F/OusW6y+RaY3/pAkSZIkSdLs7JEkSZIkSZKk\nRmxIkiRJkiRJUiM2JEmSJEmSJKkRG5IkSZIkSZLUiA1JkiRJkiRJasSGJEmSJEmSJDViQ5IkSZIk\nSZIa+f96xUi+BJ5umQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1142bfe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load train set\n",
    "data = np.loadtxt(\"datasets/dataset_2.txt\", delimiter=',', skiprows = 1)\n",
    "\n",
    "# Size of data set, and subsample (10%)\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Record size of the data set\n",
    "n = x.shape[0]\n",
    "d = x.shape[1]\n",
    "subsample_size = 100\n",
    "\n",
    "# No. of subsamples\n",
    "num_samples = 200\n",
    "    \n",
    "### Linear regression with all 5 predictors\n",
    "\n",
    "# Create a n x d array to store coefficients for 100 subsamples\n",
    "coefs_multiple = np.zeros((num_samples, d))\n",
    "\n",
    "print 'Linear regression with all predictors'\n",
    "\n",
    "# Repeat for 200 subsamples\n",
    "for i in range(num_samples):\n",
    "    # Generate a random subsample of 50 data points\n",
    "    perm = np.random.permutation(n) # Generate a list of indices 0 to n and permute it\n",
    "    x_subsample = x[perm[:subsample_size], :] # Get x-vals for the first 50 indices in permuted list\n",
    "    \n",
    "    y_subsample = y[perm[:subsample_size]] # Get y-vals for the first 50 indices in permuted list\n",
    "\n",
    "    # Fit linear regression model on subsample\n",
    "    w, c = multiple_linear_regression_fit(x_subsample, y_subsample)\n",
    "    # Store the coefficient for the model we obtain\n",
    "    coefs_multiple[i, :] = w\n",
    "\n",
    "# Plot histogram of coefficients, and report their confidence intervals \n",
    "fig, axes = plt.subplots(1, d, figsize=(20, 3))\n",
    "\n",
    "# Repeat for each coefficient\n",
    "for j in range(d):\n",
    "    # Compute mean for the j-th coefficent from subsamples\n",
    "    coef_j_mean = np.mean(coefs_multiple[:, j])\n",
    "    \n",
    "    # Compute confidence interval at 95% confidence level (use formula!)\n",
    "    conf_int_left = np.percentile(coefs_multiple[:, j], 2.5)\n",
    "    conf_int_right = np.percentile(coefs_multiple[:, j], 97.5)\n",
    "       \n",
    "    # Plot histogram of coefficient values\n",
    "    axes[j].hist(coefs_multiple[:, j], alpha=0.5)\n",
    "\n",
    "    # Plot vertical lines at mean and left, right extremes of confidence interval\n",
    "    axes[j].axvline(x = coef_j_mean, linewidth=3)\n",
    "    axes[j].axvline(x = conf_int_left, linewidth=1, c='r')\n",
    "    axes[j].axvline(x = conf_int_right, linewidth=1, c='r')\n",
    "    \n",
    "    # Set plot labels\n",
    "    axes[j].set_title('[' + str(round(conf_int_left, 4)) \n",
    "                      + ', ' \n",
    "                      + str(round(conf_int_right, 4)) + ']')\n",
    "    axes[j].set_xlabel('Predictor ' + str(j + 1))\n",
    "    axes[j].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the confidence interval for the 0 th coefficient: [ 0.552772624516 , 0.75038508161 ]\n",
      "the confidence interval for the 1 th coefficient: [ 0.352230409656 , 0.749743369436 ]\n",
      "the confidence interval for the 2 th coefficient: [ 0.0889138463556 , 0.47338926001 ]\n",
      "the confidence interval for the 3 th coefficient: [ 0.809809940402 , 1.09854837094 ]\n",
      "the confidence interval for the 4 th coefficient: [ 0.0785426153803 , 0.488433606409 ]\n"
     ]
    }
   ],
   "source": [
    "# Add column of ones to x matrix\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Create model for linear regression\n",
    "model = sm.OLS(y, x)\n",
    "# Fit model\n",
    "fitted_model = model.fit()\n",
    "# The confidence intervals for our five coefficients are contained in the last five\n",
    "# rows of the fitted_model.conf_int() array\n",
    "conf_int = fitted_model.conf_int()[1:, :]\n",
    "\n",
    "for j in range(d):\n",
    "    print 'the confidence interval for the', j, 'th coefficient: [', conf_int[j][0], ',', conf_int[j][1], ']'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Polynomial regression\n",
    "In this problem, we revisit a dataset from Homework 1 and fit polynomial regression models to it. The dataset is provided in the file ``dataset_3.txt``, which contains a single predictor variable ``x`` in the first column and the response variable ``y`` in the second column. \n",
    "\n",
    "### Part(a): Implement polynomial regression from scratch\n",
    "\n",
    "- Implement the following three functions from scratch:\n",
    "\n",
    "    - ``polynomial_regression_fit``:\n",
    "        - takes as input: training set, ``x_train``, ``y_train`` and the degree of the polynomial\n",
    "        - fits a polynomial regression model \n",
    "        - returns the model parameters (array of coefficients and the intercept)\n",
    "\n",
    "    - ``polynomial_regression_predict``: \n",
    "        - takes as input: the model parameters (array of coefficients and the intercept), the degree of the polynomial and the test set predictors ``x_test``\n",
    "        - returns the response values predicted by the model on the test set. \n",
    "\n",
    "    - ``polynomial_regression_score``: \n",
    "        - takes an array of predicted response values and the array of true response values ``y_test``\n",
    "        - returns R^2 score for the model on the test set, as well as the sum of squared errors\n",
    "\n",
    "- Fit polynomial regression models of degrees 3, 5, 10 and 25 to the data set. Visualize the original data along with the fitted models for the various degrees in the same plot. \n",
    "\n",
    "For this problem, you may either use the multiple linear regression functions implemented in the Problem 1 or use the in-built functions in ``sklearn``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.91325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727570</td>\n",
       "      <td>0.36886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.14077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.24985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163920</td>\n",
       "      <td>1.22470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x        y\n",
       "0  0.016353  0.91325\n",
       "1  0.727570  0.36886\n",
       "2  0.627700  0.14077\n",
       "3  0.832000  0.24985\n",
       "4  0.163920  1.22470"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset is provided in the file dataset_3.txt, \n",
    "# which contains a single predictor variable x in the first column and the response variable y in the second column.\n",
    "\n",
    "# Load dataset set into a pandas df\n",
    "df = pd.read_csv('datasets/dataset_3.txt')\n",
    "#data = np.loadtxt(\"datasets/dataset_3.txt\", delimiter=',', skiprows = 1)\n",
    "\n",
    "# Size of the dataframe\n",
    "print 'number of rows:', df.shape[0]\n",
    "\n",
    "# Print first 5 rows of the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f5d995c45aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Perform basic EDA on the dataset to understand it a bit better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictor variable x for response variable y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farmer/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farmer/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farmer/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farmer/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farmer/anaconda/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform basic EDA on the dataset to understand it a bit better\n",
    "plt.scatter(df['x'], df['y'])\n",
    "\n",
    "plt.title(\"Predictor variable x for response variable y\")\n",
    "plt.xlabel(\"Predictor variable x\")\n",
    "plt.ylabel(\"Response variable y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part (b): Comparing training and test errors\n",
    "\n",
    "- Split the data set in Problem 2 each into training and test sets: use the first 50% of the data for training and the remaining for testing. \n",
    "\n",
    "\n",
    "- Fit polynomial models of varying degree ranging from 1 to 15 to the training sets. Evaluate  the various fits on **both** the training and the test sets. Plot both the R^2 score of the fitted polynomial models on the training and test sets as a functions of the degree. \n",
    "\n",
    "\n",
    "- Describe the relationship between degree of the polynomial model and the fit on both the training and testing data. Explain, based on the plot, what is the best polynomial model for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Model selection criterion\n",
    "In this problem, we examine various criteria that help us decide how to choose between multiple models for the same data.\n",
    "\n",
    "###  Part (a): How does one choose the best polynomial degree?\n",
    "In Problem 2, you fitted polynomials of different degrees to the entire data set, and inspected the quality of fits on the test set. In practice, one needs to find the 'best' model for the given prediction task using **only** the training set. For this, we'll now make use of two model selection criteria, namely, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These are evaluated on the training set, but serve as a proxy for the test set accuracy.\n",
    "\n",
    "For ``dataset_3.txt``, do the following:\n",
    "\n",
    "- For each polynomial model you fitted, compute the AIC and BIC for the model on the training set. Plot the criterion values as a function of the polynomial degree.\n",
    "\n",
    "\n",
    "- Which model is chosen by each criterion? Do they match with the model that yields maximum test R^2 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Application to New York taxi cab density estimation\n",
    "\n",
    "We shall now apply the concepts learned so far to a real-world prediction task. You are asked to build a regression model for estimating the density of Green cab taxis at any given time of a day in New York city. The model needs to take the time of the day (in minutes) as input, and predict the expected number of pick ups at that time.\n",
    "\n",
    "The data set for this problem can be downloaded from the following URL: https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-01.csv. The file contains the details of all pickups by Green cabs in New York City during January 2015. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Advanced regression techniques\n",
    "In this problem, we revisit the automobile pricing data set in Problem 1(a) and explore advanced regression techniques to build better models. \n",
    "\n",
    "\n",
    "### Part (a): Polynomial regression on multi-dimensions\n",
    "In Problems 2-3, you had implemented a polynomial regression technique for data sets with a single predictor variable. How would you use a similar approach to fit a polynomial model on data sets with more than one predictor?\n",
    "\n",
    "Reload ``dataset_1_train.txt`` and ``dataset_1_test.txt``. Fit polynomial models of degrees 2 and 3 to the training set, and evaluate the R^2 score of the fitted model on the test set. How do they compare with the test performance of a linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Weighted linear regression\n",
    "\n",
    "Suppose you are told that some of the prices recorded in the training set are noisy, and you are given the list of noisy points, how would you use this information during training to fit a better regression model?\n",
    "\n",
    "The noise level for each training point is provided in the file dataset_1_train_noise_levels.txt. A noise level 'none' indicates that the price is accurate, and a noise level 'noisy' indicates that the price is only moderately accurate. \n",
    "\n",
    "We want to fit a linear regression model that accounts for this new information. One way to do this is to assign different weights to each training point based on the amount of noise associated to that training point. That is, our loss function is now\n",
    "$$\n",
    "\\sum_{i=1}^n \\alpha_i\\,(y_i \\,-\\, w^T x_i)^2\n",
    "$$\n",
    "where $\\alpha_i$ is a number representing how much you value the contribution of the data point $x_i$.\n",
    "\n",
    "How does the R^2 score (evaluated on the test set) of the new linear model compare to the one fitted using plain linear regression?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
